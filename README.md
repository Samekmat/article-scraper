# ArticleScraper

Django application for scraping articles from web pages using Selenium, with REST API for accessing scraped data.

## üìã Table of Contents

- [Features](#features)
- [Tech Stack](#tech-stack)
- [Installation](#installation)
  - [Using pip](#using-pip)
  - [Using uv (recommended)](#using-uv-recommended)
- [Configuration](#configuration)
  - [Environment Variables for Localhost](#environment-variables-for-localhost)
  - [Environment Variables for Docker](#environment-variables-for-docker)
- [Running the Project](#running-the-project)
  - [Localhost Development](#localhost-development)
  - [Docker Development](#docker-development)
- [Using the Scraper](#using-the-scraper)
- [API Endpoints](#api-endpoints)
- [Testing](#testing)
- [Project Structure](#project-structure)
- [Assumptions and Limitations](#assumptions-and-limitations)

---

<a id="features"></a>
## üöÄ Features

- **Web Scraping**: Extracts articles from web pages using Selenium (supports JavaScript-rendered content)
- **Date Parsing**: Intelligent date extraction supporting:
  - ISO 8601 formats
  - Polish and English textual dates (e.g., "10 wrze≈õnia 2024", "12 November 2023")
  - Relative dates (e.g., "3 hours ago", "2 godziny temu")
  - Meta tags (article:published_time, og:published_time)
- **REST API**: Browse and filter scraped articles
- **Duplicate Prevention**: Automatically skips already scraped URLs
- **Error Handling**: Detects and skips 404/500 error pages

---

<a id="tech-stack"></a>
## üõ† Tech Stack

- **Python 3.14+**
- **Django 5.2+**
- **Django REST Framework**
- **PostgreSQL**
- **Selenium** with Chrome/Chromium
- **Beautiful Soup 4** for HTML parsing
- **dateparser** for intelligent date parsing
- **Docker & Docker Compose** for containerization

---

<a id="installation"></a>
## üì¶ Installation

### Prerequisites

- Python 3.14+
- PostgreSQL
- Chrome/Chromium browser (for local development)

### Using pip

1. **Clone the repository:**
   ```bash
   git clone https://github.com/Samekmat/article-scraper.git
   cd ArticleScraper
   ```

2. **Create virtual environment:**
   ```bash
   python -m venv .venv
   source .venv/bin/activate  # Linux/Mac
   # or
   .venv\Scripts\activate  # Windows
   ```

3. **Install dependencies:**
   ```bash
   pip install -r requirements.txt
   ```

### Using uv (recommended)

1. **Install uv** (if not already installed):
   ```bash
   # Linux/Mac
   curl -LsSf https://astral.sh/uv/install.sh | sh
   
   # Windows
   powershell -c "irm https://astral.sh/uv/install.ps1 | iex"
   
   # Or using pip
   pip install uv
   ```

2. **Clone the repository:**
   ```bash
   git clone https://github.com/Samekmat/article-scraper.git
   cd ArticleScraper
   ```

3. **Create virtual environment and install dependencies:**
   ```bash
   uv venv
   source .venv/bin/activate  # Linux/Mac
   # or
   .venv\Scripts\activate  # Windows
   
   uv sync
   ```

4. **Add new packages (if needed):**
   ```bash
   uv add package-name
   ```

---

<a id="configuration"></a>
## ‚öôÔ∏è Configuration

### Environment Variables for Localhost

Create a \`.env\` file in the project root:

```env
# PostgreSQL Database Configuration
DB_NAME=article_db
DB_USER=article_user
DB_PASSWORD=yourpassword
DB_HOST=localhost
DB_PORT=5432

# Django Secret Key
# Generate: python -c "from django.core.management.utils import get_random_secret_key; print(get_random_secret_key())"
SECRET_KEY=your-secret-key-here

# Selenium Configuration (Local Mode)
REMOTE_SELENIUM=false

# Optional: Custom Chrome binary path
# CHROME_BINARY=/usr/bin/chromium
```

### Environment Variables for Docker

For Docker, create a \`.env\` file with these settings:

```env
# PostgreSQL Database Configuration (Docker)
DB_NAME=article_db
DB_USER=article_user
DB_PASSWORD=yourpassword
DB_HOST=db  # Docker service name
DB_PORT=5432

# Django Secret Key
SECRET_KEY=your-secret-key-here

# Selenium Configuration (Remote Mode)
REMOTE_SELENIUM=true
SELENIUM_URL=http://selenium:4444/wd/hub

# Chrome binary for Docker
CHROME_BINARY=/usr/bin/chromium
```

**Note:** You can use \`.env.dist\` as a template - copy it to \`.env\` and update values.

---

<a id="running-the-project"></a>
## üèÉ Running the Project

### Localhost Development

1. Ensure database is available and .env is configured.
- Create a PostgreSQL database and user that match values in your .env.
- You can use any tool you prefer
- Example .env keys: DB_HOST, DB_PORT, DB_NAME, DB_USER, DB_PASSWORD.

2. Apply migrations:
```bash
python manage.py migrate
```

3. Run development server:
```bash
python manage.py runserver
```

4. Access the application:
- API: http://localhost:8000/api/articles/
- Admin: http://localhost:8000/admin/

### Docker Development

1. Build and start containers:
```bash
docker-compose up --build
```

2. View logs:
```bash
docker-compose logs -f
# or only the web service
# docker-compose logs -f web
```

3. Apply database migrations:
```bash
docker-compose exec web python manage.py migrate
```

4. Run tests:
```bash
docker-compose exec web python manage.py test
```

5. Run scraper:
```bash
# Without arguments: scrapes 4 predefined task URLs
docker-compose exec web python manage.py scrape_articles

# With URLs (positional)
docker-compose exec web python manage.py scrape_articles https://example.com/a https://example.com/b

# Or with --urls flag
docker-compose exec web python manage.py scrape_articles --urls https://example.com/a https://example.com/b
```

6. Stop and remove containers:
```bash
docker-compose down
# Remove volumes (‚ö†Ô∏è deletes database data)
# docker-compose down -v
```

---

<a id="using-the-scraper"></a>
## üï∑ Using the Scraper

### Scrape Articles Command

The scraper is a Django management command. You can:
- run it with no arguments to scrape 4 predefined task URLs, or
- pass URLs as positional arguments, or
- pass URLs with the --urls flag.

```bash
# 1) No arguments ‚Üí scrapes 4 predefined task URLs
python manage.py scrape_articles

# 2) Positional URLs
python manage.py scrape_articles https://example.com/article1 https://example.com/article2

# 3) Using --urls flag
python manage.py scrape_articles --urls https://example.com/article1 https://example.com/article2

# Docker variants
docker-compose exec web python manage.py scrape_articles
docker-compose exec web python manage.py scrape_articles https://example.com/article1 https://example.com/article2
docker-compose exec web python manage.py scrape_articles --urls https://example.com/article1 https://example.com/article2
```

### What the Scraper Does

1. ‚úÖ Checks if URL already exists (skips duplicates)
2. ‚úÖ Loads page with Selenium (waits for JavaScript)
3. ‚úÖ Extracts title, content, and publication date
4. ‚úÖ Parses dates in multiple formats (Polish/English)
5. ‚úÖ Detects and skips error pages (404, 500)
6. ‚úÖ Handles timeouts and network errors
7. ‚úÖ Saves article to database
8. ‚úÖ Logs all operations to \`scraper.log\`

### Scraper Logs

Check scraper activity:
```bash
tail -f scraper.log
```

---

<a id="api-endpoints"></a>
## üåê API Endpoints

### Base URL
```
http://localhost:8000/api/
```

### 1. List All Articles

**Endpoint:** `GET /api/articles/`

**Description:** Returns list of all scraped articles

**Example Request:**
```bash
curl http://localhost:8000/api/articles/
```

**Example Response:**
```json
[
  {
    "id": 1,
    "title": "Example Article Title",
    "html_content": "<html>...</html>",
    "plain_text_content": "Article text content...",
    "source_url": "https://example.com/article",
    "published_at": "2025-10-17T00:00:00",
    "source_domain": "example.com"
  },
  {
    "id": 2,
    "title": "Another Article",
    "html_content": "<html>...</html>",
    "plain_text_content": "More content...",
    "source_url": "https://site2.com/news",
    "published_at": "2025-10-16T00:00:00",
    "source_domain": "site2.com"
  }
]
```

### 2. Filter Articles by Source Domain

**Endpoint:** `GET /api/articles/?source=<domain>`

**Description:** Returns articles from specific domain

**Example Request:**
```bash
curl http://localhost:8000/api/articles/?source=example.com
```

**Example Response:**
```json
[
  {
    "id": 1,
    "title": "Example Article Title",
    "html_content": "<html>...</html>",
    "plain_text_content": "Article text content...",
    "source_url": "https://example.com/article",
    "published_at": "2025-10-17T00:00:00",
    "source_domain": "example.com"
  }
]
```

### 3. Get Single Article

**Endpoint:** `GET /api/articles/<id>/`

**Description:** Returns details of a specific article

**Example Request:**
```bash
curl http://localhost:8000/api/articles/1/
```

**Example Response:**
```json
{
  "id": 1,
  "title": "Example Article Title",
  "html_content": "<html>...</html>",
  "plain_text_content": "Article text content...",
  "source_url": "https://example.com/article",
  "published_at": "2025-10-17T00:00:00",
  "source_domain": "example.com"
}
```

**Error Response (404):**
```json
{
  "detail": "Not found."
}
```

### API Notes

- ‚úÖ **Read-only API**: Only GET requests are supported (no POST, PUT, DELETE)
- ‚úÖ **Pagination**: Not implemented (returns all results)
- ‚úÖ **Authentication**: Not required (public API)
- ‚úÖ **Content-Type**: \`application/json\`

---

<a id="testing"></a>
## üß™ Testing

### Run All Tests

```bash
# Localhost
python manage.py test

# Docker
docker-compose exec web python manage.py test
```

---

<a id="project-structure"></a>
## üìÅ Project Structure

```text
ArticleScraper/
‚îú‚îÄ‚îÄ api/                          # REST API app
‚îÇ   ‚îú‚îÄ‚îÄ migrations/
‚îÇ   ‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test_api.py           # API endpoint tests
‚îÇ   ‚îú‚îÄ‚îÄ serializers.py            # DRF serializers
‚îÇ   ‚îú‚îÄ‚îÄ urls.py                   # API URL routing
‚îÇ   ‚îî‚îÄ‚îÄ views.py                  # API views
‚îú‚îÄ‚îÄ articles/                     # Main articles app
‚îÇ   ‚îú‚îÄ‚îÄ management/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ commands/
‚îÇ   ‚îÇ       ‚îî‚îÄ‚îÄ scrape_articles.py  # Scraper command
‚îÇ   ‚îú‚îÄ‚îÄ migrations/
‚îÇ   ‚îú‚îÄ‚îÄ tests/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ test_models.py        # Model tests
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ test_scraper.py       # Scraper tests
‚îÇ   ‚îú‚îÄ‚îÄ models.py                 # Article model
‚îÇ   ‚îú‚îÄ‚îÄ scraper.py                # Scraping logic
‚îÇ   ‚îî‚îÄ‚îÄ views.py
‚îú‚îÄ‚îÄ ArticleScraper/               # Project settings
‚îÇ   ‚îú‚îÄ‚îÄ settings.py
‚îÇ   ‚îú‚îÄ‚îÄ urls.py
‚îÇ   ‚îî‚îÄ‚îÄ wsgi.py
‚îú‚îÄ‚îÄ .env                          # Environment variables (not in git)
‚îú‚îÄ‚îÄ .env.dist                     # Environment template
‚îú‚îÄ‚îÄ .gitignore                    # Gitignore file
‚îú‚îÄ‚îÄ docker-compose.yml            # Docker Compose config
‚îú‚îÄ‚îÄ Dockerfile                    # Docker image definition
‚îú‚îÄ‚îÄ manage.py                     # Django management script
‚îú‚îÄ‚îÄ pyproject.toml                # uv dependencies
‚îú‚îÄ‚îÄ requirements.txt              # pip dependencies
‚îú‚îÄ‚îÄ scraper.log                   # Scraper logs (not in git)
‚îú‚îÄ‚îÄ README.md                     # This file
‚îî‚îÄ‚îÄ uv.lock                       # uv lock file
```

---
<a id="assumptions-and-limitations"></a>
## ‚ö†Ô∏è Assumptions and Limitations

### Assumptions

1. **PostgreSQL Database**: Project assumes PostgreSQL
2. **Chrome/Chromium**: Selenium requires Chrome or Chromium browser
3. **JavaScript Support**: Uses Selenium for JS-rendered content
4. **Date Normalization**: All dates stored as midnight (00:00:00) if time is not specified
5. **UTF-8 Encoding**: Assumes UTF-8 for all scraped content
6. **Read-Only API**: API only supports GET requests

### Limitations

1. **No Pagination**: API returns all results (may be slow for large datasets)
2. **No Rate Limiting**: No protection against API abuse
3. **No Authentication**: API is public (no user permissions)
4. **Single Scraper Instance**: No parallel/distributed scraping
5. **Timeout Fixed**: 20-second page load timeout (hardcoded)
6. **Error Detection Heuristics**: Uses keywords for 404/500 detection (may have false positives)
7. **Date Parsing**: May fail for uncommon date formats
8. **Content Length Check**: Pages < 200 characters rejected (may exclude legitimate short pages)
9. **No Retry Logic**: Failed scrapes are not retried automatically
10. **No Content Deduplication**: Only URL-based duplicate detection

### Known Issues

- **Selenium Memory**: Chrome instances may consume significant memory
- **Docker Network**: Requires proper network configuration for Selenium Grid
- **Logging**: All logs go to \`scraper.log\` (may grow large over time)

---
